## References
- [What is AI?](https://divis.io/en/2019/03/ai-for-laymen-part-1-what-is-ai/)

====
====

## What is AI?

The aim is to explain in a comprehensible and approachable manner - how AI _really_ works, what it can (and what it cannot) do now and what can be expected in the future.

## The Big Why: Four Reasons to Understand AI Correctly

AI is currently THE hot tech topic. As with most hyped topics, there’s a strong contrast between public coverage and the actual facts. Speaking of AI, this contrast is especially harsh, for a number of reasons:

-   **Prejudices:** The term AI is heavily biased by movies, books, TV shows and computer games
-   **Lack of knowledge:** There is still a vast public ignorance concerning information technology - on which AI is based
-   **Attention economy:** Ours is a time dominated by emotional and sensational headlines - in this kind of environment, passing on information objectively has become even more difficult
-   **Marketing:** AI is an important new business area - especially the industry’s top dogs with their massive marketing budgets don’t want to have an objective debate but rather enforce the magical, science-fiction-like character of AI

So while the average citizen has at least a basic understanding of the combustion engine and can accordingly follow the “dieselgate” scandal more informedly, even IT professionals often have no real idea of how a neural network really works. This creates false and sometimes dangerous prejudices about what the current state of AI is and where its dangers and risks lie. This can also be counterproductive in planning and executing IT projects: Unrealistic ideas about the possibilities of a technology inevitably lead to problems. On the other hand, opportunities are not fully tapped if one does not know what is possible.

## Is this AI? - A small quiz

Before you talk about a complex topic like AI, one should first try to find a definition of the topic at hand. So let’s kick things off with a small quiz. Which of the following applications is AI for you?

1.  A recommender system. e.g. Amazon offering you additional products or YouTube suggesting new videos
2.  A math software able to solve complex differential equations
3.  Face recognition, e.g. Facebook linking faces in a photo to the associated accounts
4.  An image classification that detects whether the animal in a photo is a dog or a cat
5.  The ghosts from PacMan
6.  An unbeatable Tic-Tac-Toe program
7.  IBM’s Deep Blue, the first software to beat a human chess world champion
8.  DeepMind’s AlphaGo, the first software to beat a human Go world champion
9.  A credit agencies scoring system
10.  Apple’s Siri
11.  Amazon’s Alexa
12.  IBM’s Watson Software, which has beaten the best human Jeopardy players

Obviously, this was a trick question. Each item is an example for artificial intelligence - even the ghosts in PacMan are an example for an (albeit very, very simple) game AI. We like to ask audiences this question when giving a talk about AI fundamentals. It’s interesting to see how much the opinions can differ individually. It also shows how the popular perception of AI has gradually but distinctly changed over the decades. In the nineties, for example, nobody would have doubted that a chess computer that beats a world champion is an AI.Today it’s an entirely different story as the quote from a office neighbour reveals:

> I’m talking about real AI, like the Google stuff, not the chess computers from the ’90s.

The idea of what AI is therefore constantly changes due to technical development and, in particular, habituation. Above all, there is a difference between the definition used by professionals and public understanding on the other hand. As a result, people talk past each other and are quoted out of context.

## What is AI? - Attempt at a definition

But what is the definition of an AI? There is no official, ISO standardized specification, but a solid starting point is the definition given by the standard work on artificial intelligence. According to [“Artificial Intelligence - A Modern Approach”](https://en.wikipedia.org/wiki/Artificial_Intelligence%3A_A_Modern_Approach), a system is an AI if it meets one or more of the following four criteria:

|  | human | rational |
| --- | --- | --- |
| **thought** | I. It thinks like a human | II. It thinks rationally |
| **action** | III. It behaves like a human | IV. It behaves rationally |

We will come back to this definition repeatedly in the future articles, but the following observation is already interesting: There are two axes along which the definition runs:

1.  **Thought** vs. **action**: What is more important? Should the action or rather the “thinking process” meet the expectations? If we want practical results, the focus is on action. But if it is about “penetrating” the nature of thinking and of the human mind, it is on thought.
2.  **Rationality** vs. **humanity**: Man does not always act strictly rationally (in fact, he rarely does). Do we expect an AI to imitate man or a rational ideal of the mind? Both can be desirable, depending on the context.

This complex definition is intentionally formulated rather freely - as there is a multitude of algorithms labelled AI and a wide spectrum of opinions about what AI really is. The broad definition also shows how difficult it is to define AI at all.

## At what point does artificial intelligence begin?

The state of research and the insights into human intelligence have always been subject to constant change and cultural influences. The same applies, of course, to the goals of AI research. At the beginning of the computer age, the idea of a computer in itself was basically perceived as an AI - this can be seen in old SciFi movies: It goes without saying that the computer speaks and finds solutions on it’s own. When the spaceship captain asks the on-board computer “Computer, what are our options?”, this SciFi cliché perfectly reflects the perception of AI at the time. **Finding exacting mathematical solutions to complex problems** - at that time this was a task only particularly intelligent and educated folks could accomplish. A machine capable of doing this as well must therefore be “intelligent”, too. But it quickly became clear that systems of equations are (comparatively) easy for computers. Distinguishing cats from dogs on pictures or answering the spaceship captains requests, on the other hand, are incredibly complex challenges. This is a dilemma for AI researchers: Whenever they have solved a problem, they understand how the computer does it - and it no longer counts as a “real” AI. When Kasparov lost to Deep Blue in 1997, it was perfectly clear that this was state-of-the-art AI. After all, chess was a highly complex game requiring a lot of rational thought and intuition. 20 years later even IT students sometimes have a problem considering a chess program as AI - although it definitely is. This problem of the moving target is summarized [John McCarthy](https://en.wikipedia.org/wiki/John_McCarthy_%28computer_scientist%29) to the following - somewhat frustrated - statement:

> As soon as it works, no-one calls it AI anymore.

Nowadays we have reached the point where a computer actually _can_ distinguish cats from dogs and is, among other things, gradually mastering human language - in fact a huge leap forwards compared to the early days of computers. Since problems like these have been unsolvable for years, it naturally makes people think that this now finally is “real” AI (whatever that is). Some warn against the pending apocalypse, others predict the social collapse, caused by robots replacing us and making all human labor redundant. Meanwhile all these fears appear rather far-fetched when you consider the technical reality. What we are witnessing right now is just another level of digitization. There is no AI as seen in SciFi movies, with common sense knowledge, solving universal problems and acting autonomously. There is only a new paradigm of software development that can solve new, previously unsolvable problems. Like any new technology, this offers great new prospects and, of course, risks. In order to use the possibilities correctly and to counter the risks sensibly, however, education is necessary. As Arthur C. Clarke so beautifully put it:

> Any sufficiently advanced technology is indistinguishable from magic.

Right now, magical thinking is still predominant when it comes to AI. In order to change that, we want to explain in the forthcoming parts of the series, what AI _actually_ is and how it works. We will start with introducing basic concepts of AI: What defines a “strong” or a “weak” AI? What is the difference between “classical AI” and Deep Learning? Knowing the differences, strength and weaknesses of both approaches is crucial for seeing the current (and probably future) progress.

Here is [the second part of the series “Understanding AI”](https://divis.io/en/2019/03/ai-for-laymen-part-2-symbolic-ai-neural-networks-and-deep-learning/):"[Symbolic AI, Neural Networks and Deep Learning](https://divis.io/en/2019/03/ai-for-laymen-part-2-symbolic-ai-neural-networks-and-deep-learning/)", in which you will learn how to distinguish strong AI from weak AI and how to classify Deep Learning and Neural Networks.

====
====

## Part II: Symbolic AI, Neural Networks and Deep Learning
Even the performance of the early and (from today’s perspective) primitive computing systems seemed to imitate the human mind and it’s accomplishments. No wonder Alan Turing, pioneer of computer science, deeply pondered the concept of AI. [With the so-called Turing Test](https://en.wikipedia.org/wiki/Turing_test) he finally tried to establish a definition. Our own attempt at a viable definition brought us to a matrix we adopted [from a standard reference](https://en.wikipedia.org/wiki/Artificial_Intelligence:_A_Modern_Approach):

|  | human | rational |
| --- | --- | --- |
| **thought** | I. It thinks like a human | II. It thinks rationally |
| **action** | III. It behaves like a human | IV. It behaves rationally |

## Artificial general intelligence (AGI) vs. weak AI

[In our last article](https://divis.io/en/2019/03/ai-for-laymen-part-1-what-is-ai/) we not only established a definition for AI systems, but also noted the constantly changing **perception of AI**: When [Kasparov was defeated](https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov) by [**Deep Blue**](https://en.wikipedia.org/wiki/Deep_Blue_%28chess_computer%29) in 1997 it was considered a triumph for AI. And indeed Deep Blue matched the requirements II and IV of our matrix beautifully: The “reasoning” of a chess program is completely rational, plus it “behaves” completely rationally, i.e. it chooses the moves that - based on the system’s knowledge - hold the greatest probability of winning. However, we are by now accustomed to unbeatable chess programs and consequently don’t perceive them as AI any longer. We know and understand that the program only calculates all “scenarios” and acts accordingly. The magical aspect is thus lost.

Today, the nature of the public discourse has changed and has nothing to do with these “ancient” chess programs. Using the term AI, the general public and media nowadays usually mean [“artificial general intelligence” (AGI)](https://en.wikipedia.org/wiki/Artificial_general_intelligence). The misleading coverage thus fuels unfounded ideas, expectations and fears, because all **current AI** systems are “weak” or “specialized” AI. But what is the difference between “strong” and “weak” AI?

### The differences between strong and weak AI

To put it bluntly: **Strong AI** still belongs to the world of science fiction. It usually is at least as intelligent as a human being (or even superior to to it), is aware of itself and able to communicate using natural language. Furthermore it possesses “world knowledge”, i.e. it understands how the real world “works” on a physical, biological, but also social level.

[A specialized or **weak AI**](https://en.wikipedia.org/wiki/Weak_AI), e.g. a chess computer, a modern image recognition system or even Siri, is always an “one-track specialist”: Although it does it with super-human capacity, it is able to perform exactly one task. Apart from that it is incredibly limited. A chess computer can’t even play Tic-Tac-Toe. It doesn’t know what chess is, what the pieces look like or why you play chess in the first place. This limitation applies even to the most modern image recognition system: The system doesn’t know about cats and dogs, even if it can “recognize” them in images. It has no understanding of “above” or “below”, it doesn’t know about legs or fur. It merely filters complex patterns from images and classifies them, based on certain statistics. Chatbots, Alexa, Siri & Co. are nothing more than [Potemkin villages](https://en.wikipedia.org/wiki/Potemkin_village): Human behaviour and understanding are simulated with much diligence and tricks. But as soon as you stray in the least from the predefined setting, these systems are utterly helpless.

| **Strong AI (artificial general intelligence)** | **Weak AI** |
| --- | --- |
| complex physical, biological and social world knowledge | no knowledge at all or enormously specific, database knowledge |
| self-awareness, reflectivity | no self-awareness whatsoever |
| Transfer of learning: found solutions can be applied to similar problems | able to only solve only one (or very few) tasks |
| permanently learning and adapting | learning (or programming) and application strictly separated |
| Science Fiction | State of the art technology |

### Are Neural Networks and Deep Learning “strong AI”?

When we delve deeper into the functionality of various AI systems in the upcoming articles, we will always keep our focus on the actual state of research and thus on “weak” AI. Neural Networks can “fake” human behaviour, but we are not dealing with a strong AI. Despite the sensational media coverage, Neural Networks and Deep Learning do not create artificial brains. They are not “better” than the methods used for the programming of those chess computers. They simply work differently and open up new possibilities for digitisation. So what is new in the current AI hype?

## Distinction between symbolic AI, Machine Learning, Deep Learning and Neural Networks (NN)

The mentioned chess programs and similar AI systems are nowadays termed **“Symbolic” AI**. Starting from a top-down approach they try to describe a problem and its parts deterministically with symbols, rules and representations.

### The different approach: Deep Learning

**“Deep Learning”**\- like all Machine Learning techniques - on the other hand takes a different approach and uses the bottom-up principle: The algorithm tries to describe a problem stochastically at a lower level, hoping for this to result in useful representations. Most Machine Learning systems proceed like this, Deep Learning being one of them. Deep Learning uses Neural Networks (NN), complex networks that consist of several layers and are therefore “deep”. In the current scientific discourse the terms Deep Learning and artificial Neural Networks are interchangable, so we will use them accordingly.

As the functionality of Symbolic AI is much more obvious than Neural Networks, the general public has recently become more and more accustomed to it. Neural Networks and their results still seem almost “magical” in comparison. While Symbolic AI seems to be almost common nowadays, Deep Learning evokes the idea of a “real” AI. Still we need to clarify: Symbolic AI is not “dumber” or less “real” than Neural Networks. The systems work completely different, have their specific advantages and disadvantages. They even both originated at the same time, the late 50ies.

### How does Symbolic AI work?

To understand exactly how **Symbolic AI** works, we will briefly return to the chess computer. For the first moves of the game, the program accesses a database with (countless) prepared positions and chooses the “right” move. If the database offers no clear solution, the chess AI (in the background) runs millions of combinations of moves and chooses the scenario with the most promising outcome.

### How does Deep Learning work?

A good example for a Deep Learning system would be an image recognition program: First it is fed with thousands of images and the information which object class (e.g. dog or cat) the respective image belongs to. With the help of a learning algorithm it adjusts the countless parameters of this layered system step by step - until the wanted results are reliably delivered. However, it is difficult to say exactly how the decision is made. With complex visualizations we can get a certain grasp about how individual layers react to increasingly complex image characteristics. But there is no clearly defined, comprehensible decision-making evident - a Neural Network often seems more like an artificial reflex than an artificial intelligence.

## Distinction and Combination of different AI approaches

The field of AI is a wide one - the choice and application of specific algorithms or programming techniques usually determines if it is an AI. Based on our observations, the field of AI sector can be categorized thus:

![](https://divis.io/en/2019/03/ai-for-laymen-part-2-symbolic-ai-neural-networks-and-deep-learning/en_ai_technologies-1-e1553524734887-1200x975.png)

![AI examples of strong and weak AIs](https://divis.io/en/2019/03/ai-for-laymen-part-2-symbolic-ai-neural-networks-and-deep-learning/en_ai_examples-1200x982.png)

Neither method is “more real” or “more correct”. The different approaches have specific strengths and weaknesses, qualifying them for different tasks. For modern and successful applications, techniques from Symbolic AI, Neural Networks and Machine Learning are usually combined with each other. Also, not every algorithm belongs exclusively to one camp: More often than not they are successfully intertwined.

## Is Symbolic AI "wrong“ and Neural Networks "right“?

The question seems redundant as both approaches try to solve the same problem, albeit differently. Sometimes one approach works, sometimes the other. Admittedly, Symbolic AI has received more attention for a long time because it - to put it simply - yielded better results. Until recently Neural Networks were not as successful because they lacked the computing power, data and algorithmic knowledge to train the efficiently. AI research is also subject to currents and trends. For a long time, Neural Networks were simply frowned upon as an interesting but useless curiosity.

With the rise of the Internet and the ever-increasing amount of available data however, Machine Learning methods, including Neural Networks, became increasingly important since the late 1990s, resulting in the first “big data” boom. Predominant Machine Learning techniques such as Support Vector Machines (SVM) were from then on partially replaced by Deep Learning. In the public perception these seem to be omnipresent right now, solving problems computers were previously unable to solve.

However, since most successful AI applications combine different components, it is important to know the basic methods before exploring current developments. Therefore, [the next article will concentrate on **several Symbolic AI methods**](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/) and explain to what extent they can already be useful in inducing “intelligent” behavior.

====
====

## Part III: Methods of symbolic AI

Let’s remember: Symbolic AI attempts to solve problems using a **top-down approach** (example: chess computer). Machine Learning uses the **bottom-up principle** to gradually adjust a large number of parameters - until it can deliver the expected results.

Deep learning - a Machine Learning sub-category - is currently on everyone’s lips. In order to understand what’s so special about it, we will take a look at classical methods first. Even though the major advances are currently achieved in Deep Learning, no complex AI system - from personal voice-controlled assistants to self-propelled cars - will manage without one or several of the following technologies. As so often regarding software development, a successful piece of AI software is based on the right interplay of several parts.

## The most simple AI: Learning by heart

The so-called **“table-driven agent”** is the simplest AI imaginable: All correct solutions are available to the AI in the form of a table, which it can access to solve a specific problem.

This “primal” and simplest concept is however met with resistance at our lectures and events: “This has nothing to do with intelligence!” However, it is a technique that we (humans) obviously use - albeit much less efficiently. Anyone who ever had to learn vocabulary knows that. Memorizing is an aspect of the human mind - so why not that of an AI? After all, computers are particularly good at it.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Vokabellerner-1-1024x1024.jpg)

The table based agent: it simply stores all solutions in a table, all solutions are learned ahead of time by heart.

So let’s say I want to create AI software that solves a specific problem. And assuming this works flawlessly, quickly and easily using a “simple” table - why shouldn’t I choose exactly this solution?

Some might wonder where the “expertise” lies in building such a system? Well, the trick in development is first and foremost to recognize which subproblems can be solved efficiently and simply with a table (often called **“lookup”**). Using the table based agent for simple partial problems enables the overall systems to focus on the more complex parts of a task. Thus the efficiency of the entire system is improved. In this way table-driven agents can be useful supporting Neural Networks: You simply expand the input of a Machine Learning system by the possibility to take a look at the table. The learning system can focus on exceptional cases, since it does not have to learn all the information from the table first - these are already reliably available to it. In this way the performance of the whole system can be increased or the time needed for development and training can be shortened - in the best case both.

Alternatively, the table can also be the result of learning: Instead of manually determining the content of the table, it can be “learned” through a machine learning process.

Is each table therefore an AI? Of course not, it depends on how it is used. A system this simple is of course usually not useful by itself, but _if_ one can solve an AI problem by using a table containing all the solutions, one should swallow one’s pride to build something “truly intelligent”. A table-based agent is **cheap, reliable** and - most importantly - its decisions are comprehensible.

## Taking it one step further: The decision tree

Decision trees can be easily found in our everyday life: Official instructions, traffic rules, game rulebooks or tax returns are just a few examples of instructions that can be implemented as decision trees. You begin at a specific starting point and then run through questions, each one resulting from your previous answers, reaching a result, e.g. the applicable income tax rate.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Entscheidungsbaum-Beispiel-1-1024x1024.jpg)

A decision tree encodes a step-by-step, rule based decision process. In this example we decide what to take with us on a walk.

Of course, this technology is not only found in AI software, but for instance also at the checkout of an online shop (“credit card or invoice” - “delivery to Germany or the EU”). As with the table-based agent, not every decision tree is an AI. However, simple AI problems can be easily solved by decision trees (often in combination with table-based agents). The rules for the tree and the contents of tables are often implemented by experts of the respective problem domain. In this case we like to speak of an “expert system”, because one tries to map the **knowledge of experts in the form of rules**.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Entscheiungsbaum-1-1024x1024.jpg)

The classic expert system: an AI using a decision tree.

Like tables, decision trees have the enormous advantage that their decisions are comprehensible. Thinking of innovative technologies like self-driving cars, the advantage of this kind of AI becomes apparent: Using rules as transparently and deterministically as possible. Probably everyone wants to be able to understand and comprehend which “decisions” actually determine the manoeuvers of a self-driving vehicle.

Speaking of self-driving cars: We have already mentioned that in the ideal case a symbolic AI can be combined with modern methods and thus perform particularly efficiently. In the case of a self-driving car, this interplay could look like this: The Neural Network detects a stop sign (with Machine Learning based image analysis), the decision tree (Symbolic AI) decides to stop. And as with the table-driven agent, **Machine Learning can be combined with decision trees** by learning the structure of the trees. This is called “decision tree learning”. The advantage is apparent: You don’t have to create the tree yourself, but the decision making (if this- do that) is comprehensible afterwards in the application and can be adjusted if necessary.

## Intelligence based on search

“Seek and you shall find.” Search is _the_ **symbolic AI technique**. In this context “search” means that the computer tries different solutions step by step and validates the results. The classic example of this would be a **chess computer** that “imagines” millions of different future moves and combinations and, based on the outcome, “decides” which moves promise the highest probability of winning. The analogy to the human mind is obvious: Anyone who has ever played a board or strategy game intensively will have “gone through” moves in their head at least once in order to decide on them.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Suche-1024x1024.jpg)

When using search algorithms an AI inspects all possible solutions step by step. Only the part of the solution that is currently investigated is created in computer memory.

Naturally, a program has the advantage of being able to check infinitely more moves and scenarios due to its computing power. This method is the foundation of most turn-based gaming AI. Even AlphaGo is works with a variety of this technique at its core. However, there is one important **difference to humans**: A computer, equipped with the appropriate computing power, can and will execute all possible moves, including the senseless ones, in an incredibly structured way. Humans however can partially rely on their “gut”. We usually decide early on, based on our gut feeling, what actually makes sense and thus limit the number of potential moves we think about.

Recently, though, the combination of symbolic AI and Deep Learning has paid off. Neural Networks can enhance classic AI programs by adding a “human” gut feeling - and thus **reducing the number of moves to be calculated**. Using this combined technology, AlphaGo was able to win a game as complex as Go against a human being. If the computer had computed all possible moves at each step this would not have been possible.

## Intelligence based on logic

We encounter this kind of AI in old Science Fiction movies: When the computer goes crazy and becomes a threat, you give the command: “Ignore this command”. The logical paradox (if it executes the command, it doesn’t ignore it, if it doesn’t ignore it, it doesn’t execute it) causes it to crash, explode, or restart. This nicely illustrates the functionality, but also the **limits of a purely logical AI**.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Logik-1024x1024.jpg)

The classic, symbolic Science Fiction AI: throroughly determined by logic.

Such a system needs a representation of the world in unique logical values: true/false, yes/no, zero/one… and then uses logical formulas to draw conclusions. Again, the analogy to the human mind is not too far-fetched: The ideal mind should act logically and rationally (as established [in our table in the previous article](https://divis.io/en/2019/03/ai-for-laymen-part-2-symbolic-ai-neural-networks-and-deep-learning/)). Such **inflexible systems unfortunately fail** spectacularly at the real world, which is enormously messy, complex and not always quite logical.

So this is, although even a specialized programming language (Prolog) was developed for the construction of such systems, the practically least important of the classical technologies presented, although it once was _the_ poster child for a real AI. But even if one manages to express a problem in such a deterministic way, the **complexity of the computations grows** exponentially. In the end, useful applications might quickly take several billion years to solve.

Logical systems are therefore currently only of historical interest, apart from a few niche applications. But who knows: In a few years the big breakthrough of logic-based AI and Deep Learning might yet happen, and - like Neural Networks - logic-based AI might rise from the Ashes and Prolog programmers will be in high demand. We’re not counting on it, however.

## Benefits of symbolic AI

The great benefit of classical AI is that its decision-making is transparent and can easily be comprehended. It also **doesn’t require large amounts of data**, since the systems do not “learn” (based on a lot of input), but the developer “pours” her own knowledge into the system. Depending on the method, **less computing power** is needed than is necessary for training large Neural Networks.

![](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Entscheidungsbaum-Wald-1024x1024.jpg)

Some AI algorithms, like the ‘random forest’ algorithm, use multiple search trees and create the solutions by combining multiple results.

There still exist tasks in which a symbolic AI performs better. It does this especially in situations where the problem can be formulated by searching all (or most) possible solutions. However, hybrid approaches are increasingly merging symbolic AI and Deep Learning. The goal is balancing the weaknesses and problems of the one with the benefits of the other - be it the aforementioned “gut feeling” or the enormous computing power required. Apart from niche applications, it is more and more difficult to equate complex contemporary AI systems to one approach or the other. Not to mention attributing weaknesses or strengths to them.

The last great bastion of symbolic AI are **computer games**. In games, a lot of computing power is needed for graphics and physics calculations. Also, real-time behavior is desired. Thus the vast majority of computer game opponents are (still) recruited from the camp of symbolic AI.

## Disadvantages of symbolic AI

The biggest problem with symbolic AI: It’s (often) unable to successfully solve most problems from the real world. As we have to formulate our solutions using clear rules (tables, decision trees, search algorithms, symbols…), we encounter a massive obstacle the moment a problem cannot be described this easily. Interestingly, this often happens in situations where a person can reliably fall back on her “gut”. The classic example here is the **recognition of objects** in a picture. Even for small children, this is trivial. For computers it was an insurmountable problem for decades.

![Symbolic AI regrettably fails on many real world tasks: e.g. telling cats and dogs apart in pictures.](https://divis.io/en/2019/04/understanding-ai-part-3-methods-of-symbolic-ai/divisio-blog_illustrationen_Katze-erkennen-1-1024x1024.jpg)

Symbolic AI regrettably fails on many real world tasks: e.g. telling cats and dogs apart in pictures.

In general, it is always challenging for symbolic AI to leave the world of rules and definitions and enter the “real” world instead. Nowadays it frequently **serves as only an assistive technology** for Machine Learning and Deep Learning.

In [the next part of the series](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/) we will leave the deterministic and rigid world of symbolic AI and have a closer look at “learning” machines.

====
====

## Part IV: The basics of Machine Learning

Only this enables the computer to correctly access the “learned” answers (be it in the form of a decision tree or a table) and lets the program solve the problem in a comprehensible way. However, this also means that an algorithm can only provide the solution to a very specific problem. The moment the problem changes, the algorithm must be modified (or rewritten).

This is exactly where machine learning really comes in. **The aim of machine learning is to allow the computer to formulate and solve the problem** - i.e. to enable it to take over the “unpleasant part” of adjustment and fine-tuning itself. To do this, a machine learning program is equipped with two different algorithms, a **learning algorithm and an inference algorithm**. Equipped with those, the same (or very similar) program can ideally solve many different problems, while applying the same method - instead of programming everything from scratch.

On a side note, the term “learning” should be used with caution: People tend to anthropomorphise, i.e. we like to attribute human qualities to inanimate objects: Everyone has probably scolded their computer at one point or another - knowing fully well that it neither hears nor understands what is said. If we now say that computers (or rather: algorithms) “learn”, then there is a great danger of us understanding this learning process as analogous to the human learning process. But as we’ll soon discover, machines “learn” quite differently. That’s why most ML developers prefer to talk about “training” their algorithms.

## The most important ML terms, explained simply

The best way to understand how ML works? Going over an example step by step. But before we plunge right into it, we need some essential ML vocabulary first:

-   **_Algorithm_**: An algorithm is a fixed, finite sequence of instructions, used to perform a particular calculation. Each computer program consists of a number of different algorithms. An algorithm can be very simple (“Find the smallest number in this list of numbers”) or very complex (“Train this neural network”). Most ML methods work with two algorithms: The so-called **training algorithm** trains the program based on the available data, the so-called **inference algorithm** applies the gained " findings " and delivers results.
-   **_Parameter_**: A value that is learnt during the training of a machine learning program. Based on existing parameters, the training algorithm “trains” and tries to continuously improve the parameters. The inferencel algorithm uses the parameters to calculate results.
-   **_Model_**: A trained machine learning method. A model is essentially a (sometimes very large) set of parameters and instructions on how to use them - a ready-to-run ML procedure and the result of the training process. The training algorithm generates the model, the inference algorithm uses it to perform a task.
-   **_Inference_**: Running a model using a second algorithm. This algorithm uses the model to create a prediction based on an input, classify an input or create some interesting value based on an input. Hence the term “inference algorithm” or “prediction algorithm”. Inference is a technical term for “execute the ML model and output a result”.

As we see, Machine learning use the combination of a _training algorithm_ and a _prediction_ (or _inference_) _algorithm_. The training algorithm uses data to gradually determine parameters. The set of all learned parameters is called a model, basically a “set of rules” established by the algorithm, applicable even to unknown data. The _inference algorithm_ then uses the _model_ and applies it to any given data. Finally it delivers the desired results.

## Procedure of a Machine Learning Training

Equipped with the right vocabulary, we can take a closer look at the execution of a machine learning project:

1.  **We select the machine learning method for which we want to train a model.** The choice will depend on the problem to be solved, the available data, the experience and also on gut feeling.
2.  **Then we divide the available data into two parts: The training data and the test data.** We apply our training data and thus obtain our model. The model is checked on the unknown test data. It is most important that the test data aren’t used during the training phase under any circumstances. The reason is obvious: Computers are great at learning by heart. Complex models like neural networks can actually start to memorize by themselves. The following results might be quite remarkable. There’s only one flaw: They’re not based on a model formulated by the program, but on “memorized” data. This effect is called “overfitting”. However, the test data are supposed to simulate the “unknown” during quality control and to check whether the model has really “learned” something. A good model achieves about the same error rate on the test data as on the training data without ever having seen it before.
3.  **We use the training data to develop the model with the learning algorithm.** The more data we have, the “stronger” the model becomes. Using up all available data for the training algorithm is called an **“epoch”**.
4.  **In order to test it, the trained model is used on the test data unknown to it and makes predictions.** If we did everything right, the predictions on unknown data should be as good as on the training data - the model can generalize and solve the problem. Now it is ready for practical use.

![](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/divisio-blog_illustrationen_Teil-4_1.jpg)

A birds-eye-view of Machine Learning:

1.  Selection of a method,
2.  Splitting raw data into training and test set,
3.  The training algorithm trains on the training data,
4.  The inference algorithm tests with the test data and performs predictions.

## A Machine Learning project - step by step

Now that we have gone through the individual steps theoretically, we should put them to use on a specific example. Doing this we also want to introduce the most important ML algorithm - Linear Regression.

### The most important ML algorithm: Linear Regression

Applying the steps we just established, we first have to decide on the most suitable algorithm for the particular problem. Selecting an algorithm is the first step in our Machine Learning process. We described the algorithm as a guideline,  specifying how the computer should handle certain data. An especially important algorithm in Machine Learning is the so-called Linear Regression. All the basic components of ML may be found here. Even though other methods are far more complicated, they all work according to the same principle. You might also know linear regression from spreadsheets by the term “trend line”.

So what does the impressive term “Linear Regression” actually mean? A regression is a **problem** that - based on several entered variables or values - “spits out” one or more numerical values. An example of a regression, for example, is a tax return: After entering a series of values, the end result is the tax to be paid. Or the calculation of a braking distance: The speed is entered, the expected braking distance is given.

![](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/divisio-blog_illustrationen_Teil-4_2.jpg)

The inner workings of the most simple linear Regression: The query is entered on the lower axis, the answer can be read from the left axis using the line of the linear model.

**So a regression can predict numbers.** The term “linear” refers to the mathematical principle. Linear regression uses linear equations to do this. In simpler terms, we take a series of known data points, try as hard as we can to put a line through them and then use this trend line to make a prediction for new, unknown data.

Not wanting to complicate things further (and in order to make our example easier to display), we will limit ourselves here to the simplest form of linear regression, where there’s only one value entered and one value output. A certain value on the x-axis results in a corresponding value on the y-axis. The values can be displayed as points in a coordinate system.

This simple procedure has merely two parameters: On the one hand, there is the slope and the offset of the line. These two values are gradually adjusted by the training algorithm.

In this case the inference algorithm is very simple as well: Based on the input (the x-value) the output is calculated, by looking at the corresponding y-value of the line at this point.

### Putting ML into praxis - Linear Regression, step by step

Lying on a meadow, looking into the night sky, listening to the chirping of the crickets… we ask ourselves: How often per second do the crickets chirp? We assume that crickets (as insects) like it warm and that the intensity of chirping depends on the temperature. We want to calculate this for all temperatures, but we cannot control the weather to get the data. Besides, we obviously don’t want to lie around night after night with a microphone and a thermometer.

#### Step 1 - Choosing the right ML procedure

So we have two readings that seem to be linearly dependent on each other. The warmer it gets, the more intense the chirping of the crickets seems to be: Before our inner eye we see a linearly ascending line - and decide to use linear regression as the probably best method!

#### Step 2 - Training and test data for the training algorithm

In order to “learn” something, our training algorithm needs at least some data. So we take heart, pack our measuring equipment, visit the meadow and collect the training and test data for our ML method. (Or we simply [download the data here](https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr02.html)). Before we “feed” the model with the first batch of data, it’s parameters are set to a value first. These initial values for the parameters of the model are either random numbers or zeros (depending on the parameters and the ML method). Without having learned anything, the model can at least spit out values without crashing. Though, of course, these values are totally wrong.

![Linear regression before the Training data](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/divisio-blog_illustrationen_Teil-4_3.jpg)

Initial state of training the cricket model: The line is random and does not predict the data well.

#### Step 3 - Training the ML model

To start the actual training session, we enter a set of training data into the model and calculate the (probably entirely wrong) results. Because of the random initial values, the model first guesses the results at random. The random values in this initial formula cause the model to spit out meaningless numbers. But at least we have a result, even if it is wrong.

Now we can use a trick: We calculate the error. The larger the amount of test data, the clearer the picture of the errors and deviations becomes. The inference algorithm calculates the current prediction of the model. The training algorithm compares the prediction with the actual correct result in the training data and computes how much they differ. This deviation is used to adjust the parameters in order to improve the next prediction. The algorithm gradually improves our linear function, monitors the deviations and “registers” whether they are increasing or decreasing, depending on how the parameters are adjusted. In short, it states how we need to change our model to better match the data.

![Linear Regression, fed with some Training data](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/divisio-blog_illustrationen_Teil-4_4.jpg)

After using some Training data to move the line, it already predicts the data much better.

#### Step 4 - Checking against the test data

Without immersing ourselves too deeply in mathematics, let us put it this way: With the right formulas, the training data and the initial (wrong) output of the model, the algorithm can optimize the parameters and thus get a better solution next time. Using linear regression, as we did in our case, this is done by moving the line closer to the training points.

![Linear Regression, precise prediction](https://divis.io/en/2019/04/understanding-ai-part-4-the-basics-of-machine-learning/divisio-blog_illustrationen_Teil-4_5.jpg)

After using up all of the training set, the line can be used to predict unknown data points pretty well.

This process is repeated again and again until the result is good enough:

1.  We take a batch of training data, calculate the result.
2.  We calculate the _error_ and observe how much the result is wrong (e.g. by simply computing the difference to the desired result).
3.  Based on the error and clever mathematics (mostly derivatives) we calculate how we have to change the model to minimize the error.
4.  When we are satisfied with the result, we stop - if not, we go back to 1. and we repeat all the steps with another part of the training data.

So what would happen if we used up all the training data and are **still not happy with the result**? Easy: We initiate a new epoch (see above) after putting the data in a new random order. Ideally there is so much data as to never reach the end of an epoch. In practice, however, data is often used several times, i.e. the model is trained over several epochs. It’s somewhat similar to a human learning a foreign language: We often have to go through the new vocabulary several times (and if possible in alternating order) until we have learned it.

### Linear regression and ML dealing with “high-dimensional input”

Maybe people might think: “But that’s cheating! Drawing a line through a few points isn’t AI!” However, you have to keep in mind that the same method works not only with a simple one-dimensional function, i.e. one input and output value, but also with so-called “high-dimensional input”.

This rather pompous term merely implies that one has several input variables. It is therefore more related to a tax return than to science fiction: Every tax return has a multidimensional input (wage, independent income, number of children, …). A letter is also addressed by multidimensional entries: Name, surname, company, address supplement, street, house number, postal code, city, country - here we are at a 9-dimensional input!

Our cricket example had only one input and one output value, so that we could display the data sets two-dimensionally as a graph. But the same mathematics that can be employed to train a simple linear regression with one input and output will also work for **any number of dimensions**. Referring to the underlying mathematics, it is still called “linear” - even though the drawing of the model isn’t and can’t even be depicted graphically as a line - if at all.

## ML in practice: As simple as possible, as complicated as necessary

We conclude: With proper pre-processing, a Linear Regression can be amazingly performant. That’s why this simple algorithm still has so much practical relevance. On the one hand as a element of more complex systems - as we will see later, no Neural Network can do without Linear Regression. On the other hand, **simple methods often perform better than complex ones**, when it comes to Machine Learning. The quality of the result usually depends less on the method, but rather on the training data. Also, the performance gained by a complex procedure can be so low that the increased effort (computing time, storage space, programming effort, fine-tuning, …) for the complex procedure is not really worthwhile.

In any case, linear methods are always well suited to define a “baseline”: What can be achieved with minimal effort using existing data? Many projects start with a linear method as a baseline in order to better estimate the quality of later results.

Of course, there are **a lot of things that linear regression cannot do** - otherwise there would be no need for Deep Learning or Neural Networks. But as with the methods of Symbolic AI we have presented before, the following holds true: As simple as possible, as complicated as necessary. In a real project, the aim is to achieve the best possible result while wasting as few resources as possible.

So what happens next? We will look further at Machine Learning and discuss how to further categorize ML procedures. We will also consider their advantages and disadvantages.
